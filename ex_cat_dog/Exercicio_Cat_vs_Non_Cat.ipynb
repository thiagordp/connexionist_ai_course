{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Cat vs Non Cat Exercise\n",
    "\n",
    "Experimente resolver o problema com um perceptron apenas (regressão logistica), uma rede de camada rasa e uma rede convolucional. Além da demonstração das redes funcionando, um Relatório completo deve ser entregue, contendo informações de Quantos e Quais experimentos foram feitos até chegar no resultado final; Como foi o treinamento; Qual a taxa de acertos da rede; A matriz de confusão, etc...\n",
    "\n",
    "## Notebook structure\n",
    "\n",
    "- Import libraries\n",
    "- Data Preparation\n",
    "- Data preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ml54yDaeeZLb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "\n",
    "import h5py\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import visualkeras\n",
    "from PIL import ImageFont\n",
    "\n",
    "import logging\n",
    "    \n",
    "for name in logging.Logger.manager.loggerDict.keys():\n",
    "    logging.getLogger(name).setLevel(logging.CRITICAL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2oib_xrMc4zE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 256\n",
    "IMAGE_DIRECTORY = './Data/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVTVMbq7c6ZU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "filename = 'train_catvnoncat.h5'\n",
    "f = h5py.File(IMAGE_DIRECTORY+filename, 'r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l0LI7ucLdNLY",
    "outputId": "c5ea9d0a-6200-4b6c-898f-17288fe63b81",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "list(f.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IDNegLEpceZ_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_train = list(f['train_set_x'])\n",
    "data_label = list(f['train_set_y'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "On-tcPX2cl_v",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_tr = np.array(data_train)\n",
    "y_tr = np.array(data_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jwIYIIrm3MRJ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "filename = 'test_catvnoncat.h5'\n",
    "f = h5py.File(IMAGE_DIRECTORY+filename, 'r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HS5l5es03UL_",
    "outputId": "e1a512bc-d5bc-4387-ff5a-3e6ad55c1433",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "list(f.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPwXmeud3VTJ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_test = list(f['test_set_x'])\n",
    "data_label_test = list(f['test_set_y'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JjvHBdxY3nKC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_test = np.array(data_test)\n",
    "y_test = np.array(data_label_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2grayscale(imgs_array):\n",
    "    #\n",
    "    # Reshaping to flatten\n",
    "    #\n",
    "    rgb_weights = [0.2989, 0.5870, 0.1140]\n",
    "\n",
    "    X_tr_new = []\n",
    "\n",
    "    for i in range(imgs_array.shape[0]):\n",
    "        new_img = np.dot(imgs_array[i], rgb_weights)\n",
    "        X_tr_new.append(new_img)\n",
    "\n",
    "    X_tr_new = np.array(X_tr_new)\n",
    "    return X_tr_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpD3hlx8dJct",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 834
    },
    "id": "NyGbrdvLdeN-",
    "outputId": "707ea0e3-582f-4ab1-eb12-b32489c3b2af",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(data_train[1].shape)\n",
    "n = 20\n",
    "plt.figure(figsize=(15, 15))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(4, 5, i + 1)\n",
    "    plt.imshow(data_train[i+10])\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "moJRsL3zfA96",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Checking class balacing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "JY46gmusdo_Q",
    "outputId": "5895f372-f2ff-473a-b9a7-bc8837713957",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "sns.countplot(y_tr, palette='twilight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Calculating class weights\n",
    "#\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "list_y_tr = list(y_tr)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced', classes=np.unique(list_y_tr), y=list_y_tr)\n",
    "print(\"Labels:\", np.unique(list_y_tr))\n",
    "print(\"Class weights:\", class_weights)\n",
    "d_class_weights = dict(enumerate(class_weights))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBU3yorygSgB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset splitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1GF7En2ngdlZ",
    "outputId": "65b6286c-c2ce-4b08-c3f6-2a83fa6a7d4e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    X_tr, y_tr, test_size=0.1, stratify=y_tr, random_state=123)\n",
    "print('Shape of train set feature', X_train.shape)\n",
    "print('Shape of validation set feature', X_val.shape)\n",
    "print('Shape of train set label', Y_train.shape)\n",
    "print('Shape of validation set label', Y_val.shape)\n",
    "\n",
    "num_classes = 2\n",
    "input_shape = (64, 64, 3)\n",
    "\n",
    "Y_train = to_categorical(Y_train, num_classes)\n",
    "Y_val = to_categorical(Y_val, num_classes)\n",
    "Y_test = to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments with Convolutional Neural Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B4YBYhwAg44N",
    "outputId": "b07fbebf-25d7-4653-d17a-ca30effdbd7e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QElacAktvfNy",
    "outputId": "2d6c1f8e-6806-4469-eec1-49f6130ccb16",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=lr),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Visualizing the network architecture\n",
    "\n",
    "Convolutional layers in yellow\n",
    "Max Pooling in red\n",
    "Dense in blue-green\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Visualizing the network\n",
    "visualkeras.layered_view(model, legend=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "89OL2Y1KwUIo",
    "outputId": "4462947e-9d0e-40bc-b617-db7fcc004c66",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=40, batch_size=16, callbacks=[early_stop],\n",
    "          class_weight=d_class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "4jf7nXlgypjp",
    "outputId": "11266d45-3f54-4998-bf05-397f035cd613",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "acc = model.history.history['accuracy']\n",
    "val_acc = model.history.history['val_accuracy']\n",
    "loss = model.history.history['loss']\n",
    "val_loss = model.history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
    "plt.title('Training and Validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2UHJwCzHwvgi",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "TrainPredictions = model.predict(X_train)\n",
    "TrainPredictions = np.argmax(TrainPredictions, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "id": "485zb_RGx4VU",
    "outputId": "727108f4-3e87-40e4-9f70-22c562a3063e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_labels = np.argmax(Y_train, axis=1)\n",
    "conf = confusion_matrix(train_labels, TrainPredictions)\n",
    "\n",
    "classes = [0, 1]\n",
    "# plot confusion matrix\n",
    "plt.imshow(conf, interpolation='nearest', cmap=plt.cm.Greens)\n",
    "plt.title(\"Train Confusion Matrix\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes)\n",
    "plt.yticks(tick_marks, classes)\n",
    "\n",
    "fmt = 'd'\n",
    "thresh = conf.max() / 2.\n",
    "for i, j in itertools.product(range(conf.shape[0]), range(conf.shape[1])):\n",
    "    plt.text(j, i, format(conf[i, j], fmt),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if conf[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6tUinoq52RNK",
    "outputId": "a96e5f4e-b6ef-4b08-eb89-fc9b5821eea3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('\\nAccuracy: {:.4f}\\n'.format(\n",
    "    accuracy_score(train_labels, TrainPredictions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k1mZW98c2smX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "TestPredictions = model.predict(X_test)\n",
    "TestPredictions = np.argmax(TestPredictions, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "id": "02eZIWfo4XTB",
    "outputId": "6cef5d8c-1caf-4a5c-e97c-15c2d794edd6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_labels = np.argmax(Y_test, axis=1)\n",
    "conf = confusion_matrix(test_labels, TestPredictions)\n",
    "\n",
    "classes = [0, 1]\n",
    "# plot confusion matrix\n",
    "plt.imshow(conf, interpolation='nearest', cmap=plt.cm.Greens)\n",
    "plt.title(\"Test Confusion Matrix\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes)\n",
    "plt.yticks(tick_marks, classes)\n",
    "\n",
    "fmt = 'd'\n",
    "thresh = conf.max() / 2.\n",
    "for i, j in itertools.product(range(conf.shape[0]), range(conf.shape[1])):\n",
    "    plt.text(j, i, format(conf[i, j], fmt),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if conf[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j5G233so4rZ7",
    "outputId": "3a9bd16f-7567-47aa-b4c0-4eff43ff9dad",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('\\nAccuracy: {:.4f}\\n'.format(\n",
    "    accuracy_score(test_labels, TestPredictions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(test_labels,TestPredictions))\n",
    "\n",
    "open(\"report_cnn.txt\", \"w+\").write(str(classification_report(test_labels,TestPredictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(confusion_matrix(test_labels, TestPredictions))\n",
    "df.to_excel(\"cm_cnn.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WivfHF9K0Q8_",
    "outputId": "aa9f322e-3bb5-4d23-b348-0bb8947b69c2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(X_test[1].shape)\n",
    "x = np.expand_dims(X_test[1], axis=0)\n",
    "print(x.shape)\n",
    "Predictions = model.predict(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "CFSXzTy75j74",
    "outputId": "5558afcf-3105-46e8-a81d-5930ae4d17fa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "n = [random.randint(0, len(data_test)) for i in range(5)]\n",
    "\n",
    "for i in n:\n",
    "    #ax = plt.subplot(4, 5, i + 1)\n",
    "    ax = plt.subplot()\n",
    "    plt.imshow(data_test[i])\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "\n",
    "    Predictions = model.predict(np.expand_dims(X_test[i], axis=0))\n",
    "    Predictions = np.argmax(Predictions, axis=1)\n",
    "    if Predictions == 0:\n",
    "        print('NAO E UM GATO')\n",
    "    else:\n",
    "        print('E UM GATO')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments with Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "IMAGE_SIZE = 256\n",
    "IMAGE_DIRECTORY = './Data/'\n",
    "\n",
    "filename = 'train_catvnoncat.h5'\n",
    "f = h5py.File(IMAGE_DIRECTORY+filename, 'r')\n",
    "\n",
    "list(f.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = list(f['train_set_x'])\n",
    "data_label = list(f['train_set_y'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "X_tr = np.array(data_train)\n",
    "y_tr = np.array(data_label)\n",
    "\n",
    "X_tr_gray = rgb2grayscale(X_tr)\n",
    "\n",
    "\n",
    "#\n",
    "# Reshaping to flatten\n",
    "#\n",
    "X_tr = X_tr_gray.reshape((X_tr_gray.shape[0], -1))\n",
    "y_tr = y_tr.reshape((y_tr.shape[0], -1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'test_catvnoncat.h5'\n",
    "f = h5py.File(IMAGE_DIRECTORY+filename, 'r')\n",
    "\n",
    "data_test = list(f['test_set_x'])\n",
    "data_label_test = list(f['test_set_y'])\n",
    "\n",
    "X_test = np.array(data_test)\n",
    "y_test = np.array(data_label_test)\n",
    "\n",
    "\n",
    "X_test_gray = rgb2grayscale(X_test)\n",
    "\n",
    "\n",
    "#\n",
    "# Reshaping to flatten\n",
    "#\n",
    "X_test = X_test_gray.reshape((X_test_gray.shape[0], -1))\n",
    "y_test = y_test.reshape((y_test.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Data scaling\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(X_tr)\n",
    "\n",
    "X_tr_scaled = scaler.transform(X_tr)\n",
    "X_test_scaled= scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "class LogisticRegressionIAC():\n",
    "\n",
    "    def __init__(self, params, input_shape) -> None:\n",
    "        self.params = params\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def search_best_hyperparams(self, X_train, y_train, class_weight):\n",
    "\n",
    "        model = LogisticRegression(\n",
    "            penalty=\"l2\",\n",
    "            max_iter=1000,\n",
    "            n_jobs=2\n",
    "        )\n",
    "        print(model.get_params().keys())\n",
    "\n",
    "        clf = GridSearchCV(model, params, cv=10,verbose=0, n_jobs=11)\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        print(\"Best params: \", clf.best_params_)\n",
    "        print(\"Best estimator:\", clf.best_estimator_)\n",
    "        with open(\"best_lr.txt\", \"w+\") as fp:\n",
    "            fp.write(\"Best params: \" +str( clf.best_params_))\n",
    "\n",
    "        return  clf.best_estimator_\n",
    "\n",
    "\n",
    "    def clear_tf(self):\n",
    "        tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training and search for the best hyperparam\n",
    "\n",
    "params = {\n",
    "    \"max_iter\": [32, 64,128,256, 512, 1024],\n",
    "    \"penalty\": [\"l2\" ,\"none\"],\n",
    "    \"C\": [0, 1e-10, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5,],\n",
    "    \"solver\":[\"lbfgs\", \"liblinear\", \"sag\", \"saga\"],\n",
    "    \"l1_ratio\":[0, 0.25, 0.5, 0.75, 1]\n",
    "}\n",
    "\n",
    "model = LogisticRegressionIAC(\n",
    "    input_shape=X_train.shape,\n",
    "    params=params\n",
    ")\n",
    "\n",
    "best_model = model.search_best_hyperparams(X_train=X_tr_scaled, y_train=y_tr, class_weight=d_class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# Make predictions using the best model\n",
    "\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(y_test,y_pred))\n",
    "open(\"report_lr.txt\", \"w+\").write(str(classification_report(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(cm)\n",
    "df.to_excel(\"cm_lr.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments with Simple NN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "IMAGE_SIZE = 256\n",
    "IMAGE_DIRECTORY = './Data/'\n",
    "\n",
    "filename = 'train_catvnoncat.h5'\n",
    "f = h5py.File(IMAGE_DIRECTORY+filename, 'r')\n",
    "\n",
    "list(f.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = list(f['train_set_x'])\n",
    "data_label = list(f['train_set_y'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "X_tr = np.array(data_train)\n",
    "y_tr = np.array(data_label)\n",
    "\n",
    "#\n",
    "# Reshaping to flatten\n",
    "#\n",
    "rgb_weights = [0.2989, 0.5870, 0.1140]\n",
    "grayscale_image = np.dot(X_tr, rgb_weights)\n",
    "\n",
    "X_tr_new = []\n",
    "\n",
    "for i in range(X_tr.shape[0]):\n",
    "    new_img = np.dot(X_tr[i], rgb_weights)\n",
    "    X_tr_new.append(new_img)\n",
    "\n",
    "X_tr_new = np.array(X_tr_new)\n",
    "X_tr_new = X_tr_new.reshape((X_tr_new.shape[0], -1))\n",
    "y_tr = y_tr.reshape((y_tr.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'test_catvnoncat.h5'\n",
    "f = h5py.File(IMAGE_DIRECTORY+filename, 'r')\n",
    "\n",
    "data_test = list(f['test_set_x'])\n",
    "data_label_test = list(f['test_set_y'])\n",
    "\n",
    "X_test = np.array(data_test)\n",
    "y_test = np.array(data_label_test)\n",
    "\n",
    "\n",
    "#\n",
    "# Reshaping to flatten\n",
    "#\n",
    "X_test_new = []\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    new_img = np.dot(X_test[i], rgb_weights)\n",
    "    X_test_new.append(new_img)\n",
    "\n",
    "X_test_new = np.array(X_test_new)\n",
    "X_test_new = X_test_new.reshape((X_test_new.shape[0], -1))\n",
    "y_test = y_test.reshape((y_test.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train shape:\", X_tr_new.shape)\n",
    "print(\"Test shape:\", X_test_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Data scaling\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(X_tr_new)\n",
    "\n",
    "X_tr_scaled = scaler.transform(X_tr_new)\n",
    "X_test_scaled= scaler.transform(X_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam, RMSprop, Adadelta, Adagrad, Adamax,Nadam\n",
    "\n",
    "\n",
    "def create_model(\n",
    "        lr=1e-3, \n",
    "        hidden_layer_size=256,\n",
    "        opt=\"adam\",\n",
    "        dropout=None,\n",
    "        hidden_activation=\"relu\",\n",
    "        input_shape=X_train.shape[1]\n",
    "    ):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "  \n",
    "\n",
    "    # Building model\n",
    "    layers = []\n",
    "    dense=Dense(hidden_layer_size, activation = hidden_activation, input_shape=(input_shape,))\n",
    "\n",
    "    layers.append(dense)\n",
    "    if dropout:\n",
    "        layers.append(Dropout(dropout))\n",
    "\n",
    "    # Output\n",
    "    layers.append(Dense(1, activation=\"sigmoid\"))\n",
    "    model = Sequential(\n",
    "       layers\n",
    "    )\n",
    "\n",
    "    metrics = [\n",
    "        keras.metrics.FalseNegatives(name=\"fn\"),\n",
    "        keras.metrics.FalsePositives(name=\"fp\"),\n",
    "        keras.metrics.TrueNegatives(name=\"tn\"),\n",
    "        keras.metrics.TruePositives(name=\"tp\"),\n",
    "        keras.metrics.Precision(name=\"precision\"),\n",
    "        keras.metrics.Recall(name=\"recall\"),\n",
    "        keras.metrics.Accuracy(name=\"accuracy\")\n",
    "    ]\n",
    "\n",
    "    optimizers = {\n",
    "        \"adam\":Adam(lr),\n",
    "        \"sgd\":SGD(lr),\n",
    "        \"rmsprop\":RMSprop(lr),\n",
    "        \"adadelta\":Adadelta(lr),\n",
    "        \"adagrad\":Adagrad(lr),\n",
    "        \"adamax\":Adamax(lr),\n",
    "        \"nadam\":Nadam(lr)\n",
    "    }\n",
    "    model.compile(\n",
    "        optimizer=optimizers[opt],  loss=\"binary_crossentropy\", metrics=metrics\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class NN_IAC():\n",
    "\n",
    "    def __init__(self, params, input_shape) -> None:\n",
    "        self.params = params\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def search_best_hyperparams(self, X_train, y_train, class_weight):\n",
    "        \n",
    "        model = KerasClassifier(\n",
    "            build_fn=create_model, \n",
    "            input_shape=X_train.shape[1], \n",
    "            epochs=200, verbose=0, \n",
    "            hidden_layer_size=256\n",
    "        )\n",
    "        \n",
    "\n",
    "        clf = GridSearchCV(model, params, verbose=4, n_jobs=1, cv=10)\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        print(\"Best params: \", clf.best_params_)\n",
    "        print(\"Best estimator:\", clf.best_estimator_)\n",
    "        with open(\"best_mlp.txt\", \"w+\") as fp:\n",
    "            fp.write(\"Best params: \" +str( clf.best_params_))\n",
    "\n",
    "        return  clf.best_estimator_\n",
    "\n",
    "\n",
    "    def clear_tf(self):\n",
    "        tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training and search for the best hyperparam\n",
    "\n",
    "params = {\n",
    "    \"epochs\": [16, 32, 64], # More than 64 epochs may generate\n",
    "    \"dropout\":[None, 0.5],\n",
    "    \"lr\": [1e-3, 1e-4],\n",
    "    \"batch_size\": [32, 64],\n",
    "    \"hidden_layer_size\": [256,512,1024],\n",
    "    \"opt\":[\"adam\", \"sgd\"],\n",
    "    \"hidden_activation\":[\"tanh\", \"sigmoid\", \"softmax\", \"selu\"],    \n",
    "}\n",
    "\n",
    "model = NN_IAC(\n",
    "    input_shape=X_tr_new.shape,\n",
    "    params=params\n",
    ")\n",
    "\n",
    "best_model = model.search_best_hyperparams(X_train=X_tr_new, y_train=y_tr, class_weight=d_class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# Make predictions using the best model\n",
    "\n",
    "y_pred = best_model.predict(X_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "open(\"report_mlp.txt\", \"w+\").write(str(classification_report(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(confusion_matrix(y_test, y_pred))\n",
    "df.to_excel(\"cm_mlp.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Exercicio Cat vs Non-Cat.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ml_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "6c79f3b749ac1c615955d5d100e4e2835a83d7f04dde803c7105836bec394313"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
