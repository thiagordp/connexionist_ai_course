{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Final Project\n",
    "\n",
    "## Basic Information\n",
    "\n",
    "\n",
    "| **Title:**       | Deep Learning and Natural Language Processing applied to the legal texts |\n",
    "|------------------|----------------------------------------------------------|\n",
    "| **Abstract:**    |                                                        |\n",
    "| **Author:**      | Thiago Raulino Dal Pont                                |\n",
    "| **Affiliation:** | Graduate Program in Automation and Systems Engineering  |\n",
    "| **Date**         | July 14, 2022                                          |\n",
    "\n",
    "\n",
    "## Goals of the project\n",
    "\n",
    "- ...\n",
    "\n",
    "## Project structure\n",
    "- Preprocessing\n",
    "- Representation\n",
    "- Modeling\n",
    "- Evaluation\n",
    "\n",
    "\n",
    "## Requirements\n",
    "\n",
    "\n",
    "``pip install -r requirements.txt``\n",
    "\n",
    "``python3 -m spacy download pt_core_news_sm``\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.modeling.util import get_class_weight\n",
    "from src.preprocessing.preprocessing_shallow_ml import PreProcessingShallowML\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from keras import Input, Model, metrics, regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Embedding, Reshape, Conv2D, MaxPooling2D, concatenate, Flatten, Dropout, Dense\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-17 20:57:16.420690: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2022-07-17 20:57:16.467101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-17 20:57:16.467485: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce 940MX computeCapability: 5.0\n",
      "coreClock: 1.189GHz coreCount: 3 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 37.33GiB/s\n",
      "2022-07-17 20:57:16.467718: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-07-17 20:57:16.469190: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2022-07-17 20:57:16.470500: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2022-07-17 20:57:16.470813: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2022-07-17 20:57:16.472248: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-07-17 20:57:16.473070: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-07-17 20:57:16.476015: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-07-17 20:57:16.476208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-17 20:57:16.476604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-17 20:57:16.476875: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset basic information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n",
      "{'labels': {'ganha': None, 'perde': None}}\n",
      "  -> Found 1044 files inside Data/final_dataset_2l_wo_result/ganha/*.txt\n",
      "  -> Found 116 files inside Data/final_dataset_2l_wo_result/perde/*.txt\n"
     ]
    }
   ],
   "source": [
    "DATASET_2CLASS_PATH = os.path.join(\"Data\", \"final_dataset_2l_wo_result\", \"\")\n",
    "\n",
    "preprocessor = PreProcessingShallowML()\n",
    "preprocessor.load_dataset(DATASET_2CLASS_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data preparation\n",
    "\n",
    "- In this project, we implemented a class to handle the text preprocessing in such a way that we can easily select distinct methods."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing corpus\n",
      "  -> Converting to lowercase\n",
      "  -> Removing HTML\n",
      "  -> Tokenizing\n",
      "  -> Removing punctuation\n",
      "  -> Removing Stopwords\n",
      "  -> Joining tokens into string\n"
     ]
    }
   ],
   "source": [
    "preprocessor.preprocess_corpus(\n",
    "    keep_raw=True,\n",
    "    lowercase=True,\n",
    "    stemming=False,\n",
    "    remove_html=True,\n",
    "    remove_punct=True,\n",
    "    remove_stopwords=True\n",
    ")\n",
    "\n",
    "dataset_shallow_ml = preprocessor.df_corpora"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing corpus\n",
      "  -> Converting to lowercase\n",
      "  -> Removing HTML\n",
      "  -> Tokenizing\n",
      "  -> Removing punctuation\n",
      "  -> Joining tokens into string\n"
     ]
    }
   ],
   "source": [
    "preprocessor.preprocess_corpus(\n",
    "    keep_raw=True,\n",
    "    lowercase=True,\n",
    "    stemming=False,\n",
    "    remove_html=True,\n",
    "    remove_punct=True,\n",
    "    remove_stopwords=False\n",
    ")\n",
    "\n",
    "dataset_dl = preprocessor.df_corpora"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Dataset splitting"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shapes:\n",
      " -> Train: X=(1044,)\ty=(1044,)\n",
      " -> Test:  X=(116,)\ty=(116,)\n"
     ]
    }
   ],
   "source": [
    "X = dataset_dl[\"processed_content\"]\n",
    "y = dataset_dl[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=123, stratify=y, shuffle=True)\n",
    "print(\"Dataset shapes:\")\n",
    "print(\" -> Train: X=%s\\ty=%s\" % (str(X_train.shape), str(y_train.shape)))\n",
    "print(\" -> Test:  X=%s\\ty=%s\" % (str(X_test.shape), str(y_test.shape)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "\n",
    "# Class weights\n",
    "class_weights = get_class_weight(y_train)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modelling with Deep Learning\n",
    "\n",
    "In this section, we apply the dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modelling with Temporal Convolutional Network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "\n",
    "EMBED_SIZE = 100\n",
    "EPOCHS=50\n",
    "BATCH_SIZE=16\n",
    "NUM_WORDS = 10000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "t = Tokenizer(oov_token='<UNK>',  num_words=NUM_WORDS)\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(X_train)\n",
    "t.word_index['<PAD>'] = 0\n",
    "word_index = t.word_index\n",
    "\n",
    "VOCAB_SIZE = len(t.word_index)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "train_sequences = t.texts_to_sequences(X_train)\n",
    "test_sequences = t.texts_to_sequences(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size=18589\n",
      "Number of Documents=1044\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size={}\".format(len(t.word_index)))\n",
    "print(\"Number of Documents={}\".format(t.document_count))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 2000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "((1044, 2000), (116, 2000))"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad dataset to a maximum review length in words\n",
    "X_train = sequence.pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test = sequence.pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_train.shape, X_test.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "num_classes=2 # positive -> 1, negative -> 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# Build the embeddings\n",
    "word_vectors = KeyedVectors.load_word2vec_format(\"Data/pre-trained_embeddings/glove.txt\", binary=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random 0.01\n"
     ]
    },
    {
     "data": {
      "text/plain": "(10000, 100)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size = min(len(word_index) + 1, NUM_WORDS)\n",
    "embedding_matrix = np.zeros((vocabulary_size, EMBED_SIZE))\n",
    "\n",
    "count = 0\n",
    "random_count = 0\n",
    "vec = np.random.rand(EMBED_SIZE)\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_SEQUENCE_LENGTH:\n",
    "        continue\n",
    "\n",
    "    count += 1\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        vec = np.random.rand(EMBED_SIZE)\n",
    "        embedding_matrix[i] = vec\n",
    "        random_count += 1\n",
    "\n",
    "\n",
    "print(\"Random %.2f\" % (random_count/count))\n",
    "embedding_matrix.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]2022-07-17 20:57:54.675994: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2022-07-17 20:57:54.698900: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2899885000 Hz\n",
      "2022-07-17 20:57:54.699598: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5596e821b470 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-07-17 20:57:54.699658: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2022-07-17 20:57:54.751875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-17 20:57:54.752113: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5596e79a2920 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-07-17 20:57:54.752188: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce 940MX, Compute Capability 5.0\n",
      "2022-07-17 20:57:54.752476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-17 20:57:54.752671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce 940MX computeCapability: 5.0\n",
      "coreClock: 1.189GHz coreCount: 3 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 37.33GiB/s\n",
      "2022-07-17 20:57:54.752765: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-07-17 20:57:54.752798: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2022-07-17 20:57:54.752827: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2022-07-17 20:57:54.752852: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2022-07-17 20:57:54.752878: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-07-17 20:57:54.752906: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-07-17 20:57:54.752933: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-07-17 20:57:54.753054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-17 20:57:54.753273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-17 20:57:54.753423: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
      "2022-07-17 20:57:54.753471: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-07-17 20:57:54.753769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-07-17 20:57:54.753795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n",
      "2022-07-17 20:57:54.753805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n",
      "2022-07-17 20:57:54.753932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-17 20:57:54.754164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-17 20:57:54.754342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1753 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0)\n",
      "2022-07-17 20:57:56.801888: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2022-07-17 20:57:56.953712: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-07-17 20:57:57.616058: W tensorflow/stream_executor/gpu/asm_compiler.cc:81] Running ptxas --version returned 256\n",
      "2022-07-17 20:57:57.668250: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 256, output: \n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "100%|██████████| 10/10 [28:17<00:00, 169.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 94.3(1.0)%\n",
      "F1:  0.76(0.04)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "accs = []\n",
    "f1s = []\n",
    "models = []\n",
    "hists = []\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "class_weights[1]=10\n",
    "\n",
    "for i in tqdm.tqdm(range(10)):\n",
    "\n",
    "\n",
    "    X_train_i, X_val_i, y_train_i, y_val_i = train_test_split(X_train, y_train, test_size=0.1, random_state=42,  stratify=y_train, shuffle=True)\n",
    "\n",
    "    # create the model\n",
    "    tf.keras.backend.clear_session()\n",
    "    embedding_layer = Embedding(vocabulary_size, EMBED_SIZE, weights=[embedding_matrix], trainable=True)\n",
    "\n",
    "    filter_sizes = [2, 3]\n",
    "    num_filters = 20\n",
    "    drop = 0.5\n",
    "\n",
    "    inputs = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "    embedding = embedding_layer(inputs)\n",
    "    reshape = Reshape((MAX_SEQUENCE_LENGTH, EMBED_SIZE, 1))(embedding)\n",
    "\n",
    "    convs = []\n",
    "    maxpools = []\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        conv = Conv2D(num_filters, (filter_size, EMBED_SIZE), activation='relu',\n",
    "                      kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "\n",
    "        maxpool = MaxPooling2D(\n",
    "            (MAX_SEQUENCE_LENGTH - filter_size + 1, 1), strides=(1, 1))(conv)\n",
    "\n",
    "        maxpools.append(maxpool)\n",
    "        convs.append(conv)\n",
    "\n",
    "    merged_tensor = concatenate(maxpools, axis=1)\n",
    "\n",
    "    flatten = Flatten()(merged_tensor)\n",
    "    # reshape = Reshape((3 * num_filters,))(flatten)\n",
    "    dropout = Dropout(drop)(flatten)\n",
    "    #conc = Dense(40)(dropout)\n",
    "    output = Dense(2, activation='softmax',\n",
    "                   kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
    "\n",
    "    # this creates a model that includes\n",
    "    model = Model(inputs, output)\n",
    "\n",
    "\n",
    "    callbacks = [EarlyStopping(monitor='val_recall', patience=10, restore_best_weights=True, mode=\"max\")]\n",
    "    opt = SGD(lr=1e-3)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy', tf.keras.metrics.Recall()])\n",
    "\n",
    "    history = model.fit(X_train_i, y_train_i,\n",
    "          validation_data=(X_val_i, y_val_i),\n",
    "          epochs=EPOCHS,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          class_weight=class_weights,\n",
    "          verbose=0,\n",
    "          callbacks=callbacks)\n",
    "\n",
    "    y_pred = model.predict(X_val_i)\n",
    "    accs.append(sklearn.metrics.accuracy_score(np.argmax(y_val_i, axis=1), np.argmax(y_pred, axis=1)))\n",
    "    f1s.append(sklearn.metrics.f1_score(np.argmax(y_val_i, axis=1), np.argmax(y_pred, axis=1)))\n",
    "    hists.append(history)\n",
    "    models.append(model)\n",
    "\n",
    "print(\"Acc: %.1f(%.1f)%%\" % (100 * np.median(accs), 100 * np.std(accs)))\n",
    "print(\"F1:  %.2f(%.2f)\" % (np.median(f1s), np.std(f1s)))\n",
    "\n",
    "best_model = models[np.argmax(f1s)]\n",
    "best_history = hists[np.argmax(f1s)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94       104\n",
      "           1       0.52      1.00      0.69        12\n",
      "\n",
      "    accuracy                           0.91       116\n",
      "   macro avg       0.76      0.95      0.81       116\n",
      "weighted avg       0.95      0.91      0.92       116\n",
      "\n",
      "[[93 11]\n",
      " [ 0 12]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(classification_report(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modeling with Shallow ML"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shapes:\n",
      " -> Train: X=(1044,)\ty=(1044,)\n",
      " -> Test:  X=(116,)\ty=(116,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = dataset_shallow_ml[\"processed_content\"]\n",
    "y = dataset_shallow_ml[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=123, stratify=y, shuffle=True)\n",
    "print(\"Dataset shapes:\")\n",
    "print(\" -> Train: X=%s\\ty=%s\" % (str(X_train.shape), str(y_train.shape)))\n",
    "print(\" -> Test:  X=%s\\ty=%s\" % (str(X_test.shape), str(y_test.shape)))\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train = le.transform(y_train)\n",
    "y_test = le.transform(y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.9, min_df=0.05, max_features=5000)\n",
    "\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "X_test_bow = vectorizer.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:18<00:00, 13.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 96.2(2.6)%\n",
      "F1:  0.78(0.19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "accs = []\n",
    "f1s = []\n",
    "models = []\n",
    "for i in tqdm.tqdm(range(10)):\n",
    "    X_train_i, X_val_i, y_train_i, y_val_i = train_test_split(X_train_bow, y_train, test_size=0.1, random_state=i,\n",
    "                                                               stratify=y_train, shuffle=True)\n",
    "\n",
    "    # Class weights\n",
    "    class_weights = get_class_weight(y_train_i)\n",
    "\n",
    "    #model = RandomForestClassifier(class_weight=class_weights, max_depth=10, n_estimators=200, n_jobs=3)\n",
    "    model = MLPClassifier(batch_size=32)\n",
    "    model.fit(X_train_i, y_train_i)\n",
    "\n",
    "    y_pred = model.predict(X_val_i)\n",
    "\n",
    "    accs.append(sklearn.metrics.accuracy_score(y_val_i, y_pred))\n",
    "    f1s.append(sklearn.metrics.f1_score(y_val_i, y_pred))\n",
    "    models.append(model)\n",
    "\n",
    "print(\"Acc: %.1f(%.1f)%%\" % (100 * np.median(accs), 100 * np.std(accs)))\n",
    "print(\"F1:  %.2f(%.2f)\" % (np.median(f1s), np.std(f1s)))\n",
    "\n",
    "best_model = models[np.argmax(f1s)]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       104\n",
      "           1       1.00      0.83      0.91        12\n",
      "\n",
      "    accuracy                           0.98       116\n",
      "   macro avg       0.99      0.92      0.95       116\n",
      "weighted avg       0.98      0.98      0.98       116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = best_model.predict(X_test_bow)\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[104   0]\n",
      " [  2  10]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('ml_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c79f3b749ac1c615955d5d100e4e2835a83d7f04dde803c7105836bec394313"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}